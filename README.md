# ë¹…ë°ì´í„°ë¶„ì„ê¸°ì‚¬ ì‹¤ê¸° ì •ë¦¬

<br>

## ğŸ’¡ ì°¸ê³  êµì¬

<div>
  <a href="https://www.dataedu.kr/">
    <img src="https://contents.kyobobook.co.kr/sih/fit-in/458x0/pdt/9791197889509.jpg" width="124px" height="180px">
  </a>
</div>

<br>

## ğŸ’¡ íŒ¨í‚¤ì§€
1. NumPy
2. Pandas
3. SciPy
4. scikit-learn
5. statsmodels

<br>

## ğŸ’¡ í™˜ê²½ ì„¤ì •
#### íŒë‹¤ìŠ¤ í–‰/ì—´ ìµœëŒ€ ì¶œë ¥
```python
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
```

#### ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
```python
import warnings
warnings.filterwarnings(action = 'ignore')
```

<br>

## ğŸ’¡ ì •ê·œì„± ê²€ì • 
#### ìƒ¤í”¼ë¡œ-ìœŒí¬ ê²€ì •
```python
from scipy.stats import shapiro
shapiro(x)
# x: 1ì°¨ì› ë°ì´í„°
```
- ê²°ê´ê°’: (í†µê³„ëŸ‰, p-value)
- p-value > 0.05ë©´ ê·€ë¬´ê°€ì„¤ ì±„íƒ >> ì •ê·œì„± ë§Œì¡±

<br>

## ğŸ’¡ ìƒê´€ë¶„ì„
#### í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
- ì—°ì†í˜• ë³€ìˆ˜ ê°„ ì¼ëŒ€ì¼ ìƒê´€ì„± í™•ì¸
```python
from scipy.stats import pearsonr
pearsonr(x, y)
# ë˜ëŠ”
df[ [x, y] ].corr()
# x, y: 1ì°¨ì› ë°ì´í„°
```
- ê²°ê´ê°’: (ìƒê´€ê³„ìˆ˜, p-value)
- p-value < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê° >> ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„

<br>

## ğŸ’¡ ì¹´ì´ì œê³± ê²€ì •
#### ì í•©ë„ ê²€ì • (chisquare)
```python
from scipy.stats import chisquare
chisquare(f_obs, f_exp)
# f_obs: ê´€ì¸¡ë„ìˆ˜
# f_exp: ê¸°ëŒ€ë„ìˆ˜
```
- ê²°ê´ê°’: (statistic, p-value)
- p-value < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

#### ë™ì§ˆì„± ê²€ì • (chi2_contigency)
```python
from scipy.stats import chi2_contingency
chi, pvalue, dof, expected = chi2_contigency(df)
# df: R X C êµì°¨í‘œë¡œ ëœ ë°ì´í„°í”„ë ˆì„
```
- ê²°ê´ê°’: ( chi(ì¹´ì´ì œê³±í†µê³„ëŸ‰), pvalue, dof(ììœ ë„), expected(ê¸°ëŒ€ë¹ˆë„) )
- p-vlaue < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

#### ë…ë¦½ì„± ê²€ì • (chi2_contigency)
```python
from scipy.stats import chi2_contigency
chi, pvalue, dof, expected = chi2_contigency(df)
# df: R X C êµì°¨í‘œë¡œ ëœ ë°ì´í„°í”„ë ˆì„
```
- ê²°ê´ê°’: ( chi(ì¹´ì´ì œê³±í†µê³„ëŸ‰), pvalue, dof(ììœ ë„), expected(ê¸°ëŒ€ë¹ˆë„) )
- p-vlaue < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

#### êµì°¨í‘œ ë§Œë“¤ê¸°
```python
import pandas as pd
pd.crosstab(index, columns, ...)
# index: êµì°¨í‘œ ìƒ í–‰ìœ¼ë¡œ ë“¤ì–´ê°ˆ ë°ì´í„°
# columns: êµì°¨í‘œ ìƒ ì—´ë¡œ ë“¤ì–´ê°ˆ ë°ì´í„°
```

<br>

## ğŸ’¡ ëª¨í‰ê· ê²€ì • T-test
#### ë‹¨ì¼í‘œë³¸ t-ê²€ì •
- ëª¨ì§‘ë‹¨ Xì˜ í‰ê· (í•˜ë‚˜ì˜ í‘œë³¸)ì— ëŒ€í•œ ê°€ì„¤ ê²€ì • (ëª¨ì§‘ë‹¨ í‰ê· ê³¼ í‘œë³¸í‰ê·  ë¹„êµ)
```python
from scipy.stats import ttest_1samp
ttest_1samp(a, popmean, alternative = 'two-sided')
# a: 1ì°¨ì› ë°ì´í„°
# popmean: ê·€ë¬´ê°€ì„¤ì—ì„œ ì„¤ì •ëœ ê°’
# alternative: ëŒ€ë¦½ê°€ì„¤ ì •ì˜
    # 'two-sided': popmeanê³¼ ë‹¤ë¦„ (default)
    # 'less': popmeanë³´ë‹¤ ì‘ìŒ
    # 'greater': popmeanë³´ë‹¤ í¼
```
- ê²°ê´ê°’: (tí†µê³„ëŸ‰ê°’, p-value)
- p-value < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

#### ëŒ€ì‘í‘œë³¸ t-ê²€ì •
- ì„œë¡œ ëŒ€ì‘ë˜ëŠ” ë‘ ì§‘ë‹¨(í‘œë³¸ í¬ê¸° ê°™ìŒ)ì˜ í‰ê· ì˜ ì°¨ì´ ê²€ì • (2ê°œì˜ ì—°ì†í˜• ë³€ìˆ˜)
- ë‘ í‘œë³¸ì´ í•˜ë‚˜ì˜ ëª¨ì§‘ë‹¨ì—ì„œ ì˜´ >> ë“±ë¶„ì‚° ê°€ì • ì„±ë¦½
```python
from scipy.stats import ttest_rel
ttest_rel(a, b, alternative = 'two-sided')
# a, b: 1ì°¨ì› ë°ì´í„°
# alternative: ëŒ€ë¦½ê°€ì„¤ ì •ì˜
    # 'two-sided': ë‘ í‘œë³¸ì˜ í‰ê· ì˜ ì°¨ì´(a-b)ê°€ 0ì´ ì•„ë‹˜ (default)
    # 'less': ë‘ í‘œë³¸ì˜ í‰ê· ì˜ ì°¨ì´(a-b)ê°€ 0ë³´ë‹¤ ì‘ìŒ
    # 'greater': ë‘ í‘œë³¸ì˜ í‰ê· ì˜ ì°¨ì´(a-b)ê°€ 0ë³´ë‹¤ í¼
```
- ê²°ê´ê°’: (tí†µê³„ëŸ‰ê°’, p-value)
- p-value < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

#### ë…ë¦½í‘œë³¸ t-ê²€ì •
- ì„œë¡œ ë…ë¦½ì¸ ë‘ ëª¨ì§‘ë‹¨ ê°„ í‰ê·  ë¹„êµ (2ê°œì˜ ë²”ì£¼í˜• ì§‘ë‹¨ì— ë”°ë¥¸ ì—°ì†í˜• ìë£Œì˜ í‰ê·  ë¹„êµ)
- ë‘ í‘œë³¸ì´ í•˜ë‚˜ì˜ ëª¨ì§‘ë‹¨ì—ì„œ ì˜´ >> ë“±ë¶„ì‚° ê°€ì • ì„±ë¦½
```python
from scipy.stats import ttest_ind
ttest_ind(a, b, equal_var = True, alternative = 'two-sided')
# a, b: 1ì°¨ì› ë°ì´í„°
# equal_var: ë“±ë¶„ì‚° ì—¬ë¶€
    # True: ë“±ë¶„ì‚° (default)
    # False: ì´ë¶„ì‚°
# alternative: ëŒ€ë¦½ê°€ì„¤ ì •ì˜
    # 'two-sided': ë‘ í‘œë³¸ì˜ í‰ê· ì˜ ì°¨ì´(a-b)ê°€ 0ì´ ì•„ë‹˜ (default)
    # 'less': ë‘ í‘œë³¸ì˜ í‰ê· ì˜ ì°¨ì´(a-b)ê°€ 0ë³´ë‹¤ ì‘ìŒ
    # 'greater': ë‘ í‘œë³¸ì˜ í‰ê· ì˜ ì°¨ì´(a-b)ê°€ 0ë³´ë‹¤ í¼
```
- ê²°ê´ê°’: (tí†µê³„ëŸ‰ê°’, p-value)
- p-value < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

## ğŸ’¡ ëª¨ë¶„ì‚° ê²€ì •
#### ë‹¨ì¼í‘œë³¸ ëª¨ë¶„ì‚° ê²€ì •
```python
from scipy.stats import chi2
chi2.cdf(x, dof)
# x: ê´€ì¸¡ê°’ (ì—¬ê¸°ì„œëŠ” ê²€ì •í†µê³„ëŸ‰)
# dof: ììœ ë„

# e.g.
# ì¢Œì¸¡ê²€ì •: Pr[chisq2(dof) < ê²€ì •í†µê³„ëŸ‰] >> chi2.cdf(x, dof)
# ìš°ì¸¡ê²€ì •: Pr[chisq2(dof) > ê²€ì •í†µê³„ëŸ‰] >> 1 - chi2.cdf(x, dof)
# ì–‘ì¸¡ê²€ì •: (2 * ì¢Œì¸¡ê²€ì •ì˜_ìœ ì˜í™•ë¥ ) ë˜ëŠ” (2 * ìš°ì¸¡ê²€ì •ì˜_ìœ ì˜í™•ë¥ ) >> 2 * chi2.cdf(x, dof)
```
- ê²°ê´ê°’ (p-value)
- p-value < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

#### ì´í‘œë³¸ì— ëŒ€í•œ ë¶„ì‚°ë¹„ ê²€ì •
```python
from scipy.stats import f
f.cdf(x, dof1, dof2)
# x: ê´€ì¸¡ê°’ (ì—¬ê¸°ì„œëŠ” ê²€ì •í†µê³„ëŸ‰ == (ì§‘ë‹¨aì˜_ë¶„ì‚° / ì§‘ë‹¨bì˜_ë¶„ì‚°)
# dof1: ììœ ë„ (ì¼ë°˜ì ìœ¼ë¡œ ë¶„ëª¨ì— ëŒ€í•œ ììœ ë„)
# dof2: ììœ ë„ (ì¼ë°˜ì ìœ¼ë¡œ ë¶„ìì— ëŒ€í•œ ììœ ë„)

# e.g.
# ì¢Œì¸¡ê²€ì •: Pr[F(dof1, dof2) < ê²€ì •í†µê³„ëŸ‰]
# ìš°ì¸¡ê²€ì •: Pr[F(dof1, dof2) > ê²€ì •í†µê³„ëŸ‰]
# ì–‘ì¸¡ê²€ì • & ê²€ì •í†µê³„ëŸ‰ì´ 1ë³´ë‹¤ ì‘ì€ ê²½ìš°: Pr[F(dof1, dof2) < ê²€ì •í†µê³„ëŸ‰] + Pr[F(dof2, dof1) > 1/ê²€ì •í†µê³„ëŸ‰]
# ì–‘ì¸¡ê²€ì • & ê²€ì •í†µê³„ëŸ‰ì´ 1ë³´ë‹¤ í° ê²½ìš°: Pr[F(dof1, dof2) > ê²€ì •í†µê³„ëŸ‰] + Pr[F(dof2, dof1) < 1/ê²€ì •í†µê³„ëŸ‰]

```
- ê²°ê´ê°’ (p-value)
- p-value < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

## ğŸ’¡ ë“±ë¶„ì‚° ê²€ì •
#### ì´í‘œë³¸ ì´ìƒì— ëŒ€í•œ Bartlett ê²€ì • (ì •ê·œì„±ì„ ì¶©ì¡±í•˜ëŠ” ë°ì´í„°ì— ëŒ€í•´)
```python
from scipy.stats import bartlett
bartlett(a, b, c)
# a, b, c: 1ì°¨ì› ë°ì´í„°
```
- ê²°ê´ê°’: (ê²€ì •í†µê³„ëŸ‰, p-value)
- p-value < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

#### ì´í‘œë³¸ ì´ìƒì— ëŒ€í•œ Levene ê²€ì • (ì •ê·œì„±ì„ ì¶©ì¡±í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì— ëŒ€í•´)
```python
from scipy.stats import levene
levene(a, b, c)
# a, b, c: 1ì°¨ì› ë°ì´í„°
```
- ê²°ê´ê°’: (ê²€ì •í†µê³„ëŸ‰, p-value)
- p-value < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

## ğŸ’¡ ë¶„ì‚°ë¶„ì„
#### ì¼ì›ë°°ì¹˜ ë¶„ì‚°ë¶„ì„
```python
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
one_way = ols('ë°˜ì‘ë³€ìˆ˜~ C(ê·¸ë£¹ë³€ìˆ˜)', data = df).fit()
# 'ë°˜ì‘ë³€ìˆ˜~ C(ê·¸ë£¹ë³€ìˆ˜)': ëª¨í˜•ì— ëŒ€í•œ ì‹
# df: ë°ì´í„°í”„ë ˆì„
anova_lm(one_way)
# one_way = ì í•©ëœ ëª¨í˜•
```
- ê²°ê´ê°’: ( dof(ììœ ë„), sum_sq(í•©ê³„ì œê³±), mean_sq(í‰ê· ì œê³±), F, Pr(>F) )
- Pr(>F) < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°
- ì‚¬í›„ê²€ì • ìˆ˜í–‰ í•„ìš”

#### ì‚¬í›„ê²€ì • (Tukeyì˜ HSD)
```python
from statsmodels.multicomp import pairwise_tukeyhsd
pairwise_tukeyhsd(endog = df['ë°˜ì‘ë³€ìˆ˜'], groups = df['ê·¸ë£¹ë³€ìˆ˜'], alpha = 0.05)
# endog: ë°˜ì‘ë³€ìˆ˜ (1ì°¨ì› ë°ì´í„°)
# groups: ê·¸ë£¹ë³€ìˆ˜ (1ì°¨ì› ë°ì´í„°)
# alpha: ìœ ì˜ìˆ˜ì¤€
```
- ê²°ê´ê°’: ( meandiff(ë‘ ê·¸ë£¹ ê°„ í‰ê·  ì°¨ì´), lower/upper(95%ì˜ ì‹ ë¢°êµ¬ê°„), reject )  
  \>\> ì‹ ë¢°êµ¬ê°„ì— 0ì„ í¬í•¨: í•´ë‹¹ ê·¸ë£¹ ê°„ í‰ê·  ì°¨ì´ê°€ ì—†ìŒ  
  \>\> ì‹ ë¢°êµ¬ê°„ì— 0ì„ í¬í•¨í•˜ì§€ ì•ŠìŒ: í•´ë‹¹ ê·¸ë£¹ ê°„ í‰ê·  ì°¨ì´ê°€ ìˆìŒ  
  \>\> rejectë¡œë„ í™•ì¸ ê°€ëŠ¥ (False- ìœ ì˜ë¯¸í•œ ì°¨ì´ ì—†ìŒ, True- ìœ ì˜ë¯¸í•œ ì°¨ì´ ìˆìŒ)

<br>

#### ì´ì›ë°°ì¹˜ ë¶„ì‚°ë¶„ì„
```python
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

# êµí˜¸ì‘ìš©ì´ ìˆëŠ” ê²½ìš°
two_way = ols('ë°˜ì‘ë³€ìˆ˜~ C(ê·¸ë£¹ë³€ìˆ˜1) + C(ê·¸ë£¹ë³€ìˆ˜2) + C(ê·¸ë£¹ë³€ìˆ˜1):C(ê·¸ë£¹ë³€ìˆ˜2)', data = df).fit()
# 'ë°˜ì‘ë³€ìˆ˜~ C(ê·¸ë£¹ë³€ìˆ˜1) + C(ê·¸ë£¹ë³€ìˆ˜2) + C(ê·¸ë£¹ë³€ìˆ˜1):C(ê·¸ë£¹ë³€ìˆ˜2)': ëª¨í˜•ì— ëŒ€í•œ ì‹
# df: ë°ì´í„°í”„ë ˆì„
anova_lm(two_way)
# two_way: ì í•©ëœ ëª¨í˜•
```
- ê²°ê´ê°’: ( dof(ììœ ë„), sum_sq(í•©ê³„ì œê³±), mean_sq(í‰ê· ì œê³±), F, Pr(>F) )
- ```C(ê·¸ë£¹ë³€ìˆ˜1):C(ê·¸ë£¹ë³€ìˆ˜2)ì˜ Pr(>F)``` > 0.05ë©´ ìƒí˜¸ì‘ìš© íš¨ê³¼ê°€ ìœ ì˜í•˜ì§€ ì•ŠìŒ >> êµí˜¸ì‘ìš© ì œì™¸í•˜ê³  ë‹¤ì‹œ ìˆ˜í–‰

```python
# êµí˜¸ì‘ìš© ì œì™¸í•˜ê³  ë‹¤ì‹œ ìˆ˜í–‰
two_way = ols('ë°˜ì‘ë³€ìˆ˜~ C(ê·¸ë£¹ë³€ìˆ˜1) + C(ê·¸ë£¹ë³€ìˆ˜2)', data = df).fit()
# 'ë°˜ì‘ë³€ìˆ˜~ C(ê·¸ë£¹ë³€ìˆ˜1) + C(ê·¸ë£¹ë³€ìˆ˜2)': ëª¨í˜•ì— ëŒ€í•œ ì‹
# df: ë°ì´í„°í”„ë ˆì„
anova_lm(two_way)
# two_way: ì í•©ëœ ëª¨í˜•
```
- Pr(>F) < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê° >> í‰ê· ì— ìœ ì˜í•œ ì°¨ì´ê°€ ìˆìŒ

<br>

## ğŸ’¡ ë¹„ëª¨ìˆ˜ ê²€ì •
#### ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜ ê²€ì • (spearmanr)
```python
from scipy.stats import spearmanr
spearmanr(a, b)
# a, b: 1ì°¨ì› ë°ì´í„°
# ë˜ëŠ”
df[ [x, y] ].corr(method = 'spearmanr')
```
- ê²°ê´ê°’: (statistic, p-value)
- p-value < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°
- ìˆœìœ„ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ê²€ì • ìˆ˜í–‰ ê°€ëŠ¥

<br>

#### ì¼„ë‹¬ì˜ íƒ€ìš° ê²€ì • (kendalltau)
```python
from scipy.stats import kendalltau
kendalltau(x, y)
# x, y: ìˆœì„œê°€ ìˆëŠ” 1ì°¨ì› ë°ì´í„°
```
- ê²°ê´ê°’: (ìƒê´€ê³„ìˆ˜, p-value)
- ìƒê´€ê³„ìˆ˜ê°€ ë‚®ìœ¼ë©´ ìˆœìœ„ ê°„ ìƒê´€ê´€ê³„ê°€ ê±°ì˜ ì—†ëŠ” ê²ƒì„
- p-value < 0.05ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

#### ìœŒì½•ìŠ¨ì˜ ë¶€í˜¸ìˆœìœ„ ê²€ì • (wilcoxon)
```python
from scipy.stats import wilcoxon
# ì¼í‘œë³¸ì¼ ê²½ìš°
wilcoxon(x - y)
# x: ìˆœì„œê°€ ìˆëŠ” 1ì°¨ì› ë°ì´í„°
# y: í‰ê·  (ì¼í‘œë³¸ì´ë¯€ë¡œ í•˜ë‚˜ì˜ ìˆ«ì)
```
- ê²°ê´ê°’: ( ê²€ì •í†µê³„ëŸ‰, p-value )
- p-value < 0.05 ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

```python
from scipy.stats import wilcoxon
# ì´í‘œë³¸ì¼ ê²½ìš°
wilcoxon(x - y)
# x, y: ìˆœì„œê°€ ìˆëŠ” 1ì°¨ì› ë°ì´í„°
```
- ê²°ê´ê°’: ( ê²€ì •í†µê³„ëŸ‰, p-value )
- p-value < 0.05 ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

#### ë§Œ-ìœ„íŠ¸ë‹ˆ U ê²€ì • (mannwhitneyu)
```python
from scipy.stats import mannwhitneyu
mannwhitneyu(x, y)
# x, y: ìˆœì„œê°€ ìˆëŠ” 1ì°¨ì› ë°ì´í„°
```
- ê²°ê´ê°’: ( ê²€ì •í†µê³„ëŸ‰, p-value )
- p-value < 0.05 ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

#### í¬ë£¨ìŠ¤ì¹¼-ì™ˆë¦¬ìŠ¤ H ê²€ì • (kruskal)
```python
from scipy.stats import kruskal
kruskal(a, b, c, ...)
# a, b, c: ìˆœì„œê°€ ìˆëŠ” 1ì°¨ì› ë°ì´í„°
```
- ê²°ê´ê°’: ( ê²€ì •í†µê³„ëŸ‰, p-value )
- p-value < 0.05 ë©´ ê·€ë¬´ê°€ì„¤ ê¸°ê°

<br>

## ğŸ’¡ ì„ í˜•íšŒê·€
#### ë‹¨ìˆœì„ í˜•íšŒê·€
```python
from scipy.stats import linregress
model = linregress(x, y)
# x, y: 1ì°¨ì› ë°ì´í„°
```
- ê²°ê´ê°’: ( slope, intercept, rvalue, pvale, stderr, intercept_stderr )  
  \>\> slope: ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ì¶”ì •ëœ íšŒê·€ê³„ìˆ˜ (beta 1)   
  \>\> intercept: ìƒìˆ˜í•­ (beta 0)  
  \>\> rvalue: ëª¨í˜•ì˜ ì„¤ëª…ë ¥ (ê²°ì •ê³„ìˆ˜)  
  \>\> pvalue: ê¸°ìš¸ê¸°ì— ëŒ€í•œ í†µê³„ì  ìœ ì˜ì„±  
  \>\> stderr & intercept_stderr: íšŒê·€ê³„ìˆ˜ë“¤ì— ëŒ€í•œ í‘œì¤€ì˜¤ì°¨  
- íšŒê·€ì‹: target = intercept + (ë…ë¦½ë³€ìˆ˜*slope)

<br>

#### ë‹¤ì¤‘ì„ í˜•íšŒê·€
```python
import statsmodels.api as sm
X = df[ ['IV1', 'IV2'] ]     # ë…ë¦½ë³€ìˆ˜
y = df['DV']                 # ì¢…ì†ë³€ìˆ˜
X = sm.add_constant(X)       # ìƒìˆ˜í•­ ì¶”ê°€
model = sm.OLS(y, X).fit()
model.summary()
```
- ê²°ê´ê°’: summary() í•¨ìˆ˜ë¡œ í™•ì¸ ê°€ëŠ¥
  \>\> const(ì ˆí¸), IVë“¤ì˜ p-value(P>|t|) < 0.05ë©´ DVì— ìœ ì˜í•œ ì˜í–¥  
  \>\> F-statisticì´ í¬ê³  Prob(F-statistic) < 0.05ë©´ ëª¨ë¸ì´ ì „ë°˜ì ìœ¼ë¡œ ìœ ì˜ë¯¸  
```python
# ë°˜ì‘ë³€ìˆ˜ì˜ ê¸°ëŒ“ê°’ì— ëŒ€í•œ ì ì¶”ì • ê°’ ê³„ì‚° (ì›ë˜ ìˆë˜ IV)
X.iloc[n]                  # në²ˆì§¸ ê´€ì¸¡ì¹˜ ë°ì´í„°ê°’ í™•ì¸
model.predict(X.iloc[n])   # në²ˆì§¸ ê´€ì¸¡ì¹˜ì— ëŒ€í•œ DVì˜ ê¸°ëŒ“ê°’ ì¶”ì •

# ë°˜ì‘ë³€ìˆ˜ì˜ ê¸°ëŒ“ê°’ ì˜ˆì¸¡ (ìƒˆë¡œìš´ IVë¥¼ ì…ë ¥)
new_data = pd.DataFrame( {'const': [ê°’], 'IV1': [ê°’], 'IV2': [ê°’]} )      # ìƒˆë¡œìš´ IV ë°°ì—´ì„ ì…ë ¥ê°’ìœ¼ë¡œ ì§€ì •
result = model.get_prediction(new_data)
result.summary_frame()
```
- ê²°ê´ê°’: summary_frame() í•¨ìˆ˜ë¡œ í™•ì¸ ê°€ëŠ¥
  \>\> mean: DVì˜ ì˜ˆì¸¡ ê¸°ëŒ“ê°’  
  \>\> mean_ci_lower & mean_ci_upper: 95%ì—ì„œ ì˜ˆì¸¡êµ¬ê°„  

<br>

## ğŸ’¡ ë¡œì§€ìŠ¤í‹±íšŒê·€
#### ë¡œì§€ìŠ¤í‹±íšŒê·€
```python
import statsmodels.api as sm
X = df[ ['IV1', 'IV2'] ]     # ë…ë¦½ë³€ìˆ˜
y = df['DV']                 # ì¢…ì†ë³€ìˆ˜
X = sm.add_constant(X)       # ìƒìˆ˜í•­ ì¶”ê°€
model = sm.GLM(y, X, family = sm.families.Binomial()).fit()
# family: ë¶„í¬ì™€ ì—°ê²°í•¨ìˆ˜ ì„¤ì • (ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” sm.families.Binomial())
model.summary()
coef = model.params
```
- ê²°ê´ê°’: summary() í•¨ìˆ˜ë¡œ í™•ì¸ ê°€ëŠ¥
  \>\> P>|z| < 0.05ë©´ const(ì ˆí¸)ì™€ IVê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸  
- model.params í•¨ìˆ˜ë¡œ const(ì ˆí¸)ì™€ IVì˜ ê°’ ë” ìì„¸íˆ í™•ì¸ ê°€ëŠ¥

<br>

#### ì˜¤ì¦ˆë¹„ ê³„ì‚°
```python
# ê·¸ë£¹Aì˜ ì˜¤ì¦ˆ ê³„ì‚°
log_odds_A = coef[â€˜constâ€™] + coef[â€˜IVâ€™]
odds_A = np.exp(log_odds_A)

# ê·¸ë£¹Bì˜ ì˜¤ì¦ˆ ê³„ì‚°
log_odds_B = coef[â€˜constâ€™]
odds_B = np.exp(log_odds_B)

# ì˜¤ì¦ˆë¹„ ê³„ì‚°
odds_ratio = odds_A / odds_B

model.deviance             # ëª¨í˜•ì˜ ì´íƒˆë„
model.null_deviance        # ì˜ëª¨í˜•(ì ˆí¸ë§Œ ìˆëŠ” ëª¨í˜•)ì˜ ì´íƒˆë„
stat = dev0 â€“ dev          # í†µê³„ëŸ‰
dof = 2 â€“ 1                # ììœ ë„ = ì í•©ëª¨í˜•ì˜_íšŒê·€ê³„ìˆ˜_ìˆ˜ â€“ ì˜ëª¨í˜•ì˜_íšŒê·€ê³„ìˆ˜_ìˆ˜

# ì¹´ì´ì œê³± í†µê³„ëŸ‰ê³¼ ììœ ë„
from scipy.stats import chi2
pval = 1 â€“ chi2.cdf(stat, dof)        # ìœ ì˜í™•ë¥ 
```
- ê²°ê´ê°’
  \>\> .null_deviance: ì˜ëª¨í˜•(ì ˆí¸ë§Œ ìˆëŠ” ëª¨í˜•)ì˜ ì´íƒˆë„  
  \>\> .summary()ì˜ Deviance ê°’ ë˜ëŠ” .deviance: ì í•©ëª¨í˜•ì˜ ì´íƒˆë„  
  \>\> ë‘˜ì˜ ì°¨ì´(null_deviance â€“ deviance)ëŠ” ììœ ë„ê°€ (ì í•©ëª¨í˜•ì˜_íšŒê·€ê³„ìˆ˜_ìˆ˜ â€“ ì˜ëª¨í˜•ì˜_íšŒê·€ê³„ìˆ˜_ìˆ˜)ì¸ ì¹´ì´ì œê³± ë¶„í¬ í•˜ì˜ ëª¨í˜• ì í•©ì„±ì„ ìœ„í•œ ê²€ì • í†µê³„ëŸ‰ì´ ë¨  
  \>\> 1 â€“ chi2.cdf(stat, dof) < 0.05ë©´ ëª¨í˜•ì´ ë°ì´í„°ë¥¼ ì˜ ì í•©í•˜ê³  ìˆìŒ  

<br>
<br>

## ğŸ’¡ ì œ 2ìœ í˜• ë¬¸ì œí’€ì´ ìˆœì„œ ì •ë¦¬
#### 01. í–‰/ì—´ ìµœëŒ€ ì¶œë ¥
```python
import pandas as pd
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
```

<br>

#### 02. df êµ¬ì¡°, ê¸°ì´ˆí†µê³„ëŸ‰ í™•ì¸
```python
df.shape           # X_train, X_test, y_train ë°ì´í„°ì…‹ ëª¨ë‘ í™•ì¸
df.info()
df.head()
df.describe()
```

<br>

#### 03. X_testì˜ ID/Date ë“± ê¸°ì¤€ì´ ë˜ëŠ” ì¹¼ëŸ¼ ë³µì‚¬ & ì‚­ì œ
```python
ID = X_test['ID'].copy()                                # ê²°ê³¼ ì œì¶œí•  ë•Œ ê¸°ì¤€ ì¹¼ëŸ¼ì´ ìˆëŠ” ê²½ìš° ë³€ìˆ˜ì— ë³µì‚¬í•˜ê¸°
X_train = X_train.drop(columns = 'ID', axis = 1)        # X_train, X_test, y_trainì—ì„œ ëª¨ë‘ ì‚­ì œ
```
- ì˜ˆì¸¡ì— ë¶ˆí•„ìš”í•œ ì¹¼ëŸ¼ë„ ì‚­ì œ

<br>

#### 04. ê²°ì¸¡ì¹˜ í™•ì¸ & ê²°ì¸¡ê°’ ì±„ìš°ê¸°
```python
X_train.isnull().sum()                                  # ê²°ì¸¡ì¹˜ í™•ì¸ (X_train, X_testì— ëª¨ë‘ ì ìš©)
# ìˆ˜ì¹˜í˜• ë³€ìˆ˜
X_train['IV'].fillna(X_train['IV'].mean())              # í‰ê· ëŒ€ì¹˜
# ëª…ëª©í˜• ë³€ìˆ˜
X_train['IV'].unique()                                  # ì¹¼ëŸ¼ì˜ ìœ ì¼ê°’ í™•ì¸
X_train['IV'].value_counts()                            # ì¹¼ëŸ¼ì˜ ìœ ì¼ê°’ë³„ ê°œìˆ˜ í™•ì¸
X_train['IV'].fillna('ëŒ€ì¹˜í• _ê°’')                        # e.g. ìµœë¹ˆê°’

# ì¡°ê±´ì— ë”°ë¼ ê²°ì¸¡ê°’ ì±„ìš°ê¸°
X_train.loc[ì¡°ê±´ì‹, 'ëŒ€ì¹˜í• _IV'] = 'ëŒ€ì¹˜í• _ê°’'
```

<br>

#### 05. ë ˆì½”ë“œ ì‚­ì œ
```python
# ì¡°ê±´ë³€ìˆ˜ë¥¼ ë§Œë“  í›„ ë ˆì½”ë“œ ì‚­ì œ
condition_na = X_train['IV'].isna()                      # ì‚­ì œí•  ë ˆì½”ë“œ ì¡°ê±´ ë³€ìˆ˜ ìƒì„± (ì—¬ê¸°ì„œëŠ” isna() í•¨ìˆ˜ë¡œ ê²°ì¸¡ì¹˜ ì‚­ì œ)
X_train = X_train[~condition_na]
y_train = y_train[~condition_na]
```

<br>

#### 06. ìë£Œí˜•ë³„ ë°ì´í„°í”„ë ˆì„ ë‚˜ëˆ„ê¸° (ìˆ˜ì¹˜í˜• & ëª…ëª©í˜•)
```python
X_train_conti = X_train.select_dtypes( ['int64', 'float64'] )         # X_testì— ëŒ€í•´ì„œë„ ìˆ˜í–‰
X_train_cate = X_train.select_dtypes('object')                        # X_testì— ëŒ€í•´ì„œë„ ìˆ˜í–‰
```
- ```.select_dtypes()```ëŠ” ì›í•˜ëŠ” dtypeì„ ê°€ì§„ ë°ì´í„°í”„ë ˆì„ë§Œ ì¶”ì¶œ

<br>

#### 07. ìˆ˜ì¹˜í˜• ì¹¼ëŸ¼ ì „ì²˜ë¦¬
```python
# ë°±ë¶„ìœ¨ ì¹¼ëŸ¼ì„ ì†Œìˆ˜ì ìœ¼ë¡œ ë³€í™˜
col_per = [ ë°±ë¶„ìœ¨ë¡œ ë˜ì–´ìˆëŠ” ì¹¼ëŸ¼ëª… ]
X_train_conti[col_per] = X_train[col_per] / 100

# ìƒê´€ê´€ê³„ í™•ì¸
X_train_conti.corr()                                         # ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ëŠ” ì¹¼ëŸ¼ì´ ìˆë‹¤ë©´ í•˜ë‚˜ ì •ë„ë§Œ ë‚¨ê¸°ê³  ì‚­ì œ
X_train_conti.drop(columns = ['ì‚­ì œí• _ì¹¼ëŸ¼'], axis = 1)

# ìë£Œí˜•ì€ ìˆ˜ì¹˜í˜•ì´ì§€ë§Œ ë‚´ìš©ì´ ëª…ëª©í˜•ì´ë¼ë©´ dtype ë³€ê²½
X_train_conti['IV'].astype('object')
```
- ë°±ë¶„ìœ¨ ì²˜ë¦¬, ìƒê´€ê´€ê³„ í™•ì¸, dtype ë³€ê²½ ë“±

<br>

#### 08. ëª…ëª©í˜• ë³€ìˆ˜ ì „ì²˜ë¦¬
```python
# íŒŒìƒë³€ìˆ˜ ë§Œë“¤ê¸°
X_train_cate['age_gp'] = X_train_cate['age'].str[0]           # e.g. ë‚˜ì´ IVë¥¼ ì—°ë ¹ëŒ€ íŒŒìƒë³€ìˆ˜ë¡œ ë§Œë“¤ê¸°

# ê³µë°± ì œê±°
X_train_cate['IV'].value_counts()                             # ì¹¼ëŸ¼ì˜ ìœ ì¼ê°’ë³„ ê°œìˆ˜ í™•ì¸
X_train_cate['IV'].str.replace(' ', '')

# ë¬¸ìì—´ì„ ë¶„ë¦¬í•´ì„œ ê°ê° í•˜ë‚˜ì˜ ì—´ë¡œ ì €ì¥
X_train_cate[ ['IV1', 'IV2'] ] = X_train_cate['IV'].str.split('ê¸°ì¤€ë¬¸ì'], expand = True)
```

<br>

#### 09. ë‘ ê°œì˜ ë°ì´í„°í”„ë ˆì„ í•©ì¹˜ê¸°
```python
X_train = pd.concat( [X_train_conti, X_train_cate], axis = 1)
X_test = pd.concat( [X_test_conti, X_test_cate], axis = 1)
```

<br>

#### 10. ë°ì´í„° ë¶„í•  (í•™ìŠµìš©/ê²€ì¦ìš©)
```python
from sklearn.model_selection import train_test_split
X_TRAIN, X_VAL, y_TRAIN, y_VAL = train_test_split(X_train,
                                                  y_train,
                                                  test_size = 0.2,     # 8:2
                                                # stratify = y_train,
                                                  random_state = 1234)

df.shape # ë¶„í• í•œ ëª¨ë“  dfì— ëŒ€í•´ shapeë¡œ í¬ê¸° í™•ì¸
```
- ```stratify```ëŠ” íƒ€ê²Ÿë³€ìˆ˜ì˜ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ dfë¥¼ ë‚˜ëˆ„ëŠ” ì˜µì…˜ (ì—°ì†í˜•ì¼ ê²½ìš° ìƒëµ)

<br>

#### 11. ì›-í•« ì¸ì½”ë”©
```python
from sklearn.preprocessing import OneHotEncoder
X_TRAIN_cate = X_TRAIN.select_dtypes('object').copy()          # X_VAL, X_testì—ì„œë„ ëª…ëª©í˜•ë§Œ ì €ì¥

enc = OneHotEncoder(handle_unknown = 'ignore', sparse = False).fit(X_TRAIN_cate)
X_TRAIN_oh = enc.transform(X_TRAIN_cate)                       # X_VAL_cateì™€ X_test_cateì— ëŒ€í•´ì„œë„ ìˆ˜í–‰
```
- ëª…ëª©í˜• ë³€ìˆ˜ì— ëŒ€í•´ì„œë§Œ ì›-í•« ì¸ì½”ë”© ìˆ˜í–‰
- ```handle_unknown='ignore'```ëŠ” trainì— ì—†ëŠ” ë ˆì´ë¸”ì´ testì— ìˆë”ë¼ë„ ëª¨ë‘ 0ì´ ë¨

<br>

#### 12. ìŠ¤ì¼€ì¼ë§ (z-score í‘œì¤€í™”)
```python
from sklearn.preprocessing import Standard Scaler
X_TRAIN_conti = X_TRAIN.select_dtypes( ['int64', 'object64'] ).copy()      # X_VAL, X_testì—ì„œë„ ìˆ˜ì¹˜í˜•ë§Œ ì €ì¥

scale = StandardScaler().fit(X_TRAIN_conti)                                # X_TRAIN_contiì— ëŒ€í•´ì„œë§Œ í‘œì¤€í™” ê°ì²´ ìƒì„±
X_TRAIN_std = scale.transform(X_TRAIN_conti)                               # X_VAL_contiì™€ X_test_contiì— ëŒ€í•´ì„œë„ ìˆ˜í–‰
```

<br>

#### 13. ì…ë ¥ ë°ì´í„°ì…‹ ì¤€ë¹„
```python
# ë‘ ê°œì˜ ë°ì´í„° (_ohì™€ _std) í•©ì¹˜ê¸°
X_TRAIN = np.concatenate( [X_TRAIN_oh, X_TRAIN_std], axis = 1)
X_VAL = np.concatenate( [X_VAL_oh, X_VAL_std], axis = 1)
X_test = np.concatenate( [X_test_oh, X_test_std], axis = 1)

# yë°ì´í„°ì—ì„œ yesë¥¼ 1ë¡œ, noë¥¼ 0ìœ¼ë¡œ ë§¤í•‘ (ì—°ì†í˜• ë³€ìˆ˜ê°€ ì•„ë‹ˆë¼ë©´)
y_TRAIN = y_TRAIN['DV'].map( {'No': 0, 'Yes': 1} )
y_VAL = y_VAL['DV'].map( {'No': 0, 'Yes': 1} )

# 1ì°¨ì› ë„˜íŒŒì´ ë°°ì—´ë¡œ í‰íƒ„í™”
y_TRAIN = y_TRAIN.values.ravel()
y_VAL = y_VAL.values.ravel()
```

<br>

#### 14. ëª¨ë¸í•™ìŠµ (ëœë¤í¬ë ˆìŠ¤íŠ¸)
```python
from sklearn.ensemble import RandomForestClassifier, RandomForestRegresor

# Classifier (ë²”ì£¼í˜• ëª©í‘œë³€ìˆ˜: ì´ì§„ë¶„ë¥˜, ë‹¤ì¤‘í´ë˜ìŠ¤ ë¶„ë¥˜)
rf = RandomForestClassifier( n_estimators = 500,
                             max_depth = 3,
                             min_samples_leaf = 10,
                             max_features = 'sqrt',
                             random_state = 1234)
model_rf = rf.fit(X_TRAIN, y_TRAIN)

# Regressor (ì—°ì†í˜• ëª©í‘œë³€ìˆ˜ ì˜ˆì¸¡)
rf = RandomForestRegressor( n_estimators = 500,
                            max_depth = 3,
                            min_samples_leaf = 10,
                            max_features = 50,
                            random_state = 1234)
model_rf = rf.fit(X_TRAIN, y_TRAIN)
```

<br>

#### 15. ì„±ëŠ¥í‰ê°€
```python
# AUC (ì´ì§„ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥í‰ê°€)
from sklearn.metrics import roc_curve, auc

score_rf = model_rf.predict_proba(X_VAL)[:, 1]               # predict_probaëŠ” í™•ë¥ ê°’ì„ ì˜ˆì¸¡
fpr, tpr, thresholds = roc_curve(y_VAL, score_rf)
AUC = auc(fpr, tpr)

# RMSE (íšŒê·€ëª¨ë¸ ì„±ëŠ¥í‰ê°€)
from sklearn.metrics import mean_squared_error
pred_rf = model_rf.predict(X_VAL)                            # predictëŠ” í™•ë¥ ê°’ ê¸°ë°˜ í´ë˜ìŠ¤ ì˜ˆì¸¡ 
rmse = mean_squared_error(y_VAL, pred_rf, squared = False)
```

<br>

#### 16. ì˜ˆì¸¡ê°’ ìƒì„±
```python
# AUC
y_score = model_rf.predict_proba(X_test)[:, 1]
# RMSE
y_pred = model_rf.predict(X_test)
```

<br>

#### 17. ê²°ê³¼ ì œì¶œ
```python
obj = { 'ID': ID,               # ì œì¶œ í˜•ì‹ì— IDê°€ ìˆëŠ” ê²½ìš°
        'DV': y_score }         # ë˜ëŠ” y_pred
result = pd.DataFrame(obj)

result.to_csv('íŒŒì¼ì´ë¦„.csv', index = False)
```











